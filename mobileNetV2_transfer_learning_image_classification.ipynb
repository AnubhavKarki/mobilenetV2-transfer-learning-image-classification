{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-end Multi-class Dog Breed Classification\n",
        "\n",
        "This notebook builds an en-to-end multi-class image classifier using TensorFlow 2.0 and TensorFlow Hub.\n",
        "\n",
        "## 1. Problem\n",
        "Identifying the breed of a dog given an image of a dog.\n",
        "\n",
        "When I'm sitting at the cafe and I take a photo of a dog, I want to know what breed of dog it is.\n",
        "\n",
        "## 2. Data\n",
        "\n",
        "The data being used is from Kaggle's dog breed identification competition.\n",
        "\n",
        "## 3. Evaluation\n",
        "The evaluation is a file with prediction probabilities for each dog breed of each test image.\n",
        "\n",
        "## 4. Features\n",
        "Some information about the data:\n",
        "* Dealing with images: Unstructured Data, best to use Deep Learning / Transfer Learning\n",
        "* There are 120 breeds of dogs, hence 120 classes\n",
        "* There are around 10k + images in the test and training set\n",
        "* Training set has been provided with labels for each image, and the label for test set is to be predicted."
      ],
      "metadata": {
        "id": "y8yOG-4OWfnq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ9TZgBVZuMg"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Load the drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change Directory\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow tensorflow-hub keras tf-keras -q"
      ],
      "metadata": {
        "id": "w3fqpTTKtSwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack the dataset\n",
        "# !unzip \"/content/drive/MyDrive/Colab Notebooks/data/dog-breed-identification.zip\" -d \"/content/drive/MyDrive/Colab Notebooks/data/\""
      ],
      "metadata": {
        "id": "I0e3eaOX2vCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(tf.__version__)\n",
        "print(hub.__version__)\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"GPU\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"not available\")\n"
      ],
      "metadata": {
        "id": "Ca3t5JAR200E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting our data ready (turning into Tensors)\n",
        "With all machine learning models, our data has to be in numerical format. Turning our images into Tensors (numerical representations)."
      ],
      "metadata": {
        "id": "xi81pkO4kXVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout the labels of our data\n",
        "import pandas as pd\n",
        "labels_csv = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/labels.csv\")\n",
        "\n",
        "labels_csv.describe()"
      ],
      "metadata": {
        "id": "pD4-b7nJX90M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_csv.head()"
      ],
      "metadata": {
        "id": "oGTvzkk9lSzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count images per class\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels_csv[\"breed\"].value_counts().plot.bar(figsize=(20, 10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "99XBwg9AlbF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(labels_csv[\"breed\"].value_counts().median(), 2)"
      ],
      "metadata": {
        "id": "gAOLOzU8lrXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image('train/fff43b07992508bc822f33d8ffd902ae.jpg')"
      ],
      "metadata": {
        "id": "KaxcU22gDZcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting images and their labels\n",
        "Let's get a list of all of our image file pathnames"
      ],
      "metadata": {
        "id": "unCqHxblFzPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_csv.head()"
      ],
      "metadata": {
        "id": "VUdWQWQxEngB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pathnames from Image ID's\n",
        "import os\n",
        "\n",
        "ROOT_PATH = r\"/content/drive/MyDrive/Colab Notebooks/data/train\"\n",
        "labels_csv[\"pathname\"] = ROOT_PATH + \"/\" + labels_csv[\"id\"]\n",
        "labels_csv.head()\n"
      ],
      "metadata": {
        "id": "ItPMKBlOE7Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathname_list = list(labels_csv[\"pathname\"] + \".jpg\")\n",
        "labels_csv.drop(\"pathname\", axis=1, inplace=True)\n",
        "\n",
        "pathname_list[:5]"
      ],
      "metadata": {
        "id": "B_nBDXseGkCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_csv.head()"
      ],
      "metadata": {
        "id": "LJenIdDZGzot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of filename matches image files\n",
        "import os\n",
        "\n",
        "print(len(os.listdir(ROOT_PATH)) == len(pathname_list))"
      ],
      "metadata": {
        "id": "4oXlPRORG3uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "labels = np.array(labels_csv[\"breed\"])\n",
        "labels[:5]"
      ],
      "metadata": {
        "id": "vOOV-eOlHmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels) == len(pathname_list)"
      ],
      "metadata": {
        "id": "LnHxjSAMI4DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find unique label values\n",
        "unique_breeds = np.unique(labels)"
      ],
      "metadata": {
        "id": "Pz_DVAlIJK-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_breeds)"
      ],
      "metadata": {
        "id": "praIggG0JlxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a single label into an array of booleans\n",
        "print(labels[0])\n",
        "labels[0] == unique_breeds"
      ],
      "metadata": {
        "id": "pfVbPajSKTmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_of_boolean_labels = [x == unique_breeds for x in labels]\n",
        "array_of_boolean_labels[:2]"
      ],
      "metadata": {
        "id": "NV5-uKptK2LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(array_of_boolean_labels)"
      ],
      "metadata": {
        "id": "emITIMm6LLc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turning boolean array into integers\n",
        "print(labels[0])\n",
        "print(np.where(unique_breeds == labels[0])) # index where label occurs\n",
        "print(array_of_boolean_labels[0].argmax())\n",
        "print(array_of_boolean_labels[0].astype(int))"
      ],
      "metadata": {
        "id": "yBxiHW-iMeIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_of_integers = np.array([x.argmax() for x in array_of_boolean_labels])"
      ],
      "metadata": {
        "id": "n1wf3IYdM-7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_of_integers[0]"
      ],
      "metadata": {
        "id": "ChlGN5D2Nq3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot = np.eye(len(unique_breeds))[array_of_integers]\n",
        "one_hot[0]"
      ],
      "metadata": {
        "id": "dLWqDmxtO_qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our train / validation / test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = pathname_list\n",
        "y = array_of_boolean_labels\n",
        "\n",
        "NUM_IMAGES = 200\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X[:NUM_IMAGES],\n",
        "    y[:NUM_IMAGES],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        "    )\n"
      ],
      "metadata": {
        "id": "jKOUvibNPU5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Images (turning Images into Tensors)\n",
        "To preprocess images into Tensors -> need a function that does:\n",
        "1. Take image filepath as input\n",
        "2. Use TensorFlow to read the file and save it to a variable\n",
        "3. Turn our image (a jpg) into Tensors\n",
        "4. Normalize the image (convert color channel values from 0-255 to 0-1)\n",
        "5. Resize the image to be a shape of (224, 224)\n",
        "6. Return the modified image"
      ],
      "metadata": {
        "id": "pb39qhSmqTDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image to NumPy array\n",
        "from matplotlib.pyplot import imread\n",
        "\n",
        "image = imread(pathname_list[0])\n",
        "image"
      ],
      "metadata": {
        "id": "ACpdxxE0bZIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NumPy array to tensor\n",
        "tf.constant(image)[:2]"
      ],
      "metadata": {
        "id": "cN5CEwbEfaaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image size\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create a function for preprocessing image\n",
        "def process_image(image_path):\n",
        "  \"\"\"\n",
        "  Takes an image file path and turns it into a Tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  # Read in an image file\n",
        "  image = tf.io.read_file(image_path)\n",
        "\n",
        "  # Turn jpg image to numerical Tensor with 3 colour channels (RGB)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "  # Convert the colour channel values from 0-255 to 0-1 values\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  # Resize the image to desired value (224,224)\n",
        "  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "7GcsnAU4P8AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-Batch\n",
        "\n",
        "Why turn our data into batches?\n",
        "> When processing large sets of images / data in one go, entire dataset may not fit into the memory, hence pass them in batches."
      ],
      "metadata": {
        "id": "rEKDeraZSKqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to return a tuple (image, label)\n",
        "\n",
        "def get_image_label(image_path, label):\n",
        "  \"\"\"\n",
        "  Takes an image file path name and the associated label,\n",
        "  processes the image and returns a tuple of (image, label).\n",
        "  \"\"\"\n",
        "  image = process_image(image_path)\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "AHco2WwWR2DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size, 32 is default\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create a function to turn data into batches\n",
        "def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n",
        "  \"\"\"\n",
        "  Creates batches of data out of image (X) and label (y) pairs.\n",
        "  Shuffles the data if it's training data but doesn't shuffle if it's validation data.\n",
        "  Also accepts test data as inpu (no labels).\n",
        "  \"\"\"\n",
        "  # If the data is test dataset, we don't have labels\n",
        "  if test_data:\n",
        "    print(\"Creating test data batches\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths (no labels)\n",
        "    data_batch = data.map(process_image).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "\n",
        "  # If the data is a valid dataset, we don't need to shuffle it\n",
        "  elif valid_data:\n",
        "    print(\"Creating validation data batches...\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),  # filepaths\n",
        "                                               tf.constant(y))) # labels\n",
        "    data_batch = data.map(get_image_label).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "  else:\n",
        "    print(\"Creating training batches...\")\n",
        "    # Turn filepaths and labels into Tensors\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                               tf.constant(y)))\n",
        "    # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images\n",
        "    data = data.shuffle(buffer_size=len(X))\n",
        "\n",
        "    # Create (image,label) tuples (this also turns the image path in a preprocessed image)\n",
        "    data_batch = data.map(get_image_label).batch(BATCH_SIZE)\n",
        "\n",
        "  return data_batch"
      ],
      "metadata": {
        "id": "2tg_Mjn7S-Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation data batches\n",
        "train_data = create_data_batches(X_train, y_train)\n",
        "valid_data = create_data_batches(X_valid, y_valid, valid_data=True)"
      ],
      "metadata": {
        "id": "R5ER4NpKVcjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check different attributes of our data batches\n",
        "train_data.element_spec, valid_data.element_spec"
      ],
      "metadata": {
        "id": "sCQdOnmUVk_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Data Batches\n",
        "\n",
        "Data are now in batches, however, these can be a little hard to comprehend, visualising it..."
      ],
      "metadata": {
        "id": "AByvX2i1WC-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a function for viewing images in a data batch\n",
        "def show_25_images(images, labels):\n",
        "  \"\"\"\n",
        "  Displays a plot of 25 images and their labels from a data batch.\n",
        "  \"\"\"\n",
        "  # Setup the figure\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  # Loop through 25 (for displaying 25 images)\n",
        "  for i in range(25):\n",
        "    # Create subplots (5 rows, 5 columns)\n",
        "    ax = plt.subplot(5,5,i+1)\n",
        "    # Display an image\n",
        "    plt.imshow(images[i])\n",
        "    # Add the image label as the title\n",
        "    plt.title(unique_breeds[labels[i].argmax()])\n",
        "    # Turn the grid lanes off\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Hjrsgy-hVsZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator to iterator\n",
        "train_images, train_labels = next(train_data.as_numpy_iterator())\n",
        "# Visualize the data in a batch\n",
        "show_25_images(train_images, train_labels)"
      ],
      "metadata": {
        "id": "zcAObzC9W5V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set\n",
        "val_images, val_labels = next(valid_data.as_numpy_iterator())\n",
        "show_25_images(val_images, val_labels)"
      ],
      "metadata": {
        "id": "m95rTqjYY7kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a model\n",
        "\n",
        "Before a model is built, few things to note:\n",
        "* Input shape\n",
        "* Output shape\n",
        "* URL of the model (Transfer Learning)\n"
      ],
      "metadata": {
        "id": "SWZVnIskZ4WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup input shape to the model\n",
        "INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels\n",
        "\n",
        "# Setup output shape of our model\n",
        "OUTPUT_SHAPE = len(unique_breeds)\n",
        "\n",
        "# Setup model URL from TensorFlow Hub\n",
        "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\""
      ],
      "metadata": {
        "id": "iEAREol1ZUw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Keras API\n",
        "\n",
        "Now we've got our inputs, outputs and model ready to go.\n",
        "Let's put them together into a Keras deep learning model.\n",
        "\n",
        "Knowing this, let's create a function which:\n",
        "* Takes the input shape, output shape and the model we've chosen as parameters\n",
        "* Defines the layers in a Keras model in sequential fashion\n",
        "* Compiles the model\n",
        "* Builds the model\n",
        "* Returns the model"
      ],
      "metadata": {
        "id": "wIs685jInOTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(img_size=224, output_shape=OUTPUT_SHAPE):\n",
        "    # Build MobileNetV2 without the top layer\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=(img_size, img_size, 3),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # Build your classifier on top\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(output_shape, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "-Qd2-STmaTmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ELXzwqBy7jIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Callbacks\n",
        "Callbacks are helper functions a model can use during training to do such things as save its progress, check its progress or stop training early if a model stops improving.\n",
        "\n",
        "Two callbacks,\n",
        "1. TensorBoard which helps track model progress\n",
        "2. For early stopping which prevents our model from training for too long"
      ],
      "metadata": {
        "id": "NtQtwxew3Pze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorBoard callback\n",
        "\n",
        "There are three required steps:\n",
        "1. Load the TensorBoard notebook extension\n",
        "2. Create a TensorBoard callback which is able to save logs to a directory and pass it to our model's `fit()` function.\n",
        "3. Visualize our models training logs with the `%tensorboard` magic function (we'll do this after model training)."
      ],
      "metadata": {
        "id": "uRcaEpGR4OhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "-xEVQt5gqILe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# Create TensorBoard callback\n",
        "def create_tensorboard_callback():\n",
        "  \"\"\"\n",
        "    Create a log directory for storing TensorBoard logs\n",
        "  \"\"\"\n",
        "  logdir = os.path.join(\"/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  return tf.keras.callbacks.TensorBoard(logdir)"
      ],
      "metadata": {
        "id": "6aupwl6o4ULt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping Callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3\n",
        ")"
      ],
      "metadata": {
        "id": "5NGKiozy5OQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a model (on subset of data)\n",
        "Our first model is only going to train on 1000 images, to validate the pipeline."
      ],
      "metadata": {
        "id": "ltAdI26E6L_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10 #@param {type:\"slider\", min:10, max:1000}"
      ],
      "metadata": {
        "id": "Dy8tLC6i5qqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "print(\"GPU\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"not available\")"
      ],
      "metadata": {
        "id": "onpOMAXR6WWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to test a model\n",
        "* Create a model using `create_model()`\n",
        "* Setup a TensorBoard callback using `create_tensorboard_callback()`\n",
        "* Call the fit `fit()` function on our model passing it the training data, validation data, number of epochs to train for (`NUM_EPOCHS`) and the callbacks we'd like to use\n",
        "* Return the model"
      ],
      "metadata": {
        "id": "akVPJmv86s7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a function to train and return a trained model\n",
        "def train_model():\n",
        "  \"\"\"\n",
        "  Trains a given model and returns the trained version.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a model\n",
        "  model = create_model()\n",
        "\n",
        "  # Create new TensorBoard session everytime we train a model\n",
        "  tensorboard = create_tensorboard_callback()\n",
        "\n",
        "  # Fit the model to the data passing it the callbacks we created\n",
        "  model.fit(x=train_data,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_data=valid_data,\n",
        "            validation_freq=1,\n",
        "            callbacks=[tensorboard, early_stopping])\n",
        "\n",
        "  # Return the fitted model\n",
        "  return model"
      ],
      "metadata": {
        "id": "dHIEUnKU6pfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the data\n",
        "model = train_model()"
      ],
      "metadata": {
        "id": "obIPtJds7lGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the TensorBoard logs\n",
        "%tensorboard --logdir /logs"
      ],
      "metadata": {
        "id": "IlW1o3r6Dm_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make predictions on the validation data"
      ],
      "metadata": {
        "id": "mter694NFeOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(valid_data, verbose=1)\n",
        "predictions"
      ],
      "metadata": {
        "id": "IuAqs2ZrD9iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "id": "6ekcPynuFqEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(predictions[0])"
      ],
      "metadata": {
        "id": "d8IfQbHAFuaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index=0\n",
        "print(predictions[0])\n",
        "print(f\"Max value (probability of prediction): {np.max(predictions[0])}\")\n",
        "print(f\"Sum: {np.sum(predictions[index])}\")\n",
        "print(f\"Max index: {np.argmax(predictions[index])}\")\n",
        "print(f\"Predicted label: {unique_breeds[np.argmax(predictions[index])]}\")"
      ],
      "metadata": {
        "id": "sYKV5xSgGbDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilities into their respective label\n",
        "def get_pred_label(prediction_probabilities):\n",
        "  \"\"\"\n",
        "  Turns an array of prediction probabilities into a label.\n",
        "  \"\"\"\n",
        "  return unique_breeds[np.argmax(prediction_probabilities)]\n",
        "\n",
        "# Get a predicted label based on an array of prediction probabilities\n",
        "pred_label = get_pred_label(predictions[3])\n",
        "pred_label"
      ],
      "metadata": {
        "id": "KyY_N-ceHcx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation data is still in a batch dataset, so unbatch them to make predictions on the validation images and compare them to the validation labels (truth labels)."
      ],
      "metadata": {
        "id": "BR6ZUH1NKLLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unbatch_dataset(data):\n",
        "  # Unbatch the data\n",
        "  images_ = []\n",
        "  labels_ = []\n",
        "\n",
        "  # Loop through unbatched data\n",
        "  for image,label in data.unbatch().as_numpy_iterator():\n",
        "    images_.append(image)\n",
        "    labels_.append(unique_breeds[np.argmax(label)])\n",
        "\n",
        "  return images_, labels_"
      ],
      "metadata": {
        "id": "dewAev0qJ8YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unbatch\n",
        "val_images, val_labels = unbatch_dataset(valid_data)"
      ],
      "metadata": {
        "id": "qoQWlxD8LU-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_images[0], val_labels[0]"
      ],
      "metadata": {
        "id": "sTABD4d3Ll9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize predictions\n",
        "* Take an array of prediction probabilities, an array of ground truth, and an array of images and integers\n",
        "* Convert the prediction probabilities to a predicted label.\n",
        "* Plot the predicted label, its predicted probability, the truth label and the target image on a single plot."
      ],
      "metadata": {
        "id": "zruTIbv4NFGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation\n",
        "def plot_pred(prediction_probabilities, labels, images, n=2):\n",
        "  \"\"\"\n",
        "  View the prediction, ground truth label and image for sample n.\n",
        "  \"\"\"\n",
        "  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]\n",
        "\n",
        "  # Get the pred label\n",
        "  pred_label = get_pred_label(pred_prob)\n",
        "\n",
        "  # Plot image & remove ticks\n",
        "  plt.imshow(image)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  # Change the colour of the title depending on right or wrong\n",
        "  if pred_label == true_label:\n",
        "    color = \"green\"\n",
        "  else:\n",
        "    color = \"red\"\n",
        "\n",
        "  plt.title(\"{} {:2.0f}% {}\".format(pred_label, np.max(pred_prob)*100, true_label), color=color)"
      ],
      "metadata": {
        "id": "FYLwXxbDLpgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images, n=3)"
      ],
      "metadata": {
        "id": "D0jBkF-7M8G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confidence Threshold"
      ],
      "metadata": {
        "id": "v4JSXuujTLCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pred_conf(prediction_probabilities, labels, n=1):\n",
        "  \"\"\"\n",
        "  Plus the top 10 highest prediction confidences along with the truth label for sample n.\n",
        "  \"\"\"\n",
        "  pred_prob, true_label = prediction_probabilities[n], labels[n]\n",
        "\n",
        "  # Getthe predicted label\n",
        "  pred_label = get_pred_label(pred_prob)\n",
        "\n",
        "  # Find the top 10 prediction confidence indexes\n",
        "  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]\n",
        "\n",
        "  # Find the top 10 prediction confidence values\n",
        "  top_10_pred_values = pred_prob[top_10_pred_indexes]\n",
        "\n",
        "  # Find the top 10 prediction labels\n",
        "  top_10_pred_labels = unique_breeds[top_10_pred_indexes]\n",
        "\n",
        "  # Setup plot\n",
        "  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),\n",
        "                     top_10_pred_values,\n",
        "                     color=\"grey\")\n",
        "  plt.xticks(np.arange(len(top_10_pred_labels)),\n",
        "             labels=top_10_pred_labels,\n",
        "             rotation=\"vertical\")\n",
        "\n",
        "  # Change color of true label\n",
        "  if np.isin(true_label, top_10_pred_labels):\n",
        "    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color(\"green\")\n",
        "  else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "7b2sBvSOOPVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, n=3)"
      ],
      "metadata": {
        "id": "xQwO-pi0Ti4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out a few predictions and their different values\n",
        "i_multiplier = 0\n",
        "num_rows = 3\n",
        "num_cols = 2\n",
        "num_images = num_rows * num_cols\n",
        "plt.figure(figsize=(10 * num_cols, 5 * num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_pred(predictions, val_labels, val_images, i+i_multiplier)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, n=i+i_multiplier)\n",
        "plt.tight_layout(h_pad=1.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FIQ3atW_WA_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "iQhKMKW9ggIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "# import itertools\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10,8)):\n",
        "\n"
      ],
      "metadata": {
        "id": "i3Iq31fHgi1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "tWaGFtZhgWp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, suffix=None):\n",
        "  \"\"\"\n",
        "  Saves a given model in a models directory and appends a suffix (string).\n",
        "  \"\"\"\n",
        "  # Create a model directory pathname with current time\n",
        "  model_dir = os.path.join(\"models\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  model_path = model_dir + \".h5\" # save format of model\n",
        "  print(f\"Saving model to: {model_path}\")\n",
        "\n",
        "  model.save(model_path)\n",
        "  return model_path"
      ],
      "metadata": {
        "id": "ft_OhhyUdb8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model"
      ],
      "metadata": {
        "id": "Qi2kZdM-kvrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "  \"\"\"\n",
        "  Loads a saved model from a specified path.\n",
        "  \"\"\"\n",
        "  print(f\"Loading saved model from: {model_path}\")\n",
        "  model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "6O5m01H5kxU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "save_model(model, suffix=\"mobilenetv2-Adam\")"
      ],
      "metadata": {
        "id": "P3LjL1Axks--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_model = load_model('models/20260203-034454.h5')"
      ],
      "metadata": {
        "id": "iiGPZF8gmJkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction on Test Set"
      ],
      "metadata": {
        "id": "0XjeThUmoVC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test image filename\n",
        "test_path = 'test/'\n",
        "test_filenames = [test_path + fname for fname in os.listdir(test_path)]\n",
        "len(test_filenames)"
      ],
      "metadata": {
        "id": "7wXEsOCKmQs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test data batch\n",
        "test_data = create_data_batches(test_filenames, test_data=True)"
      ],
      "metadata": {
        "id": "grBdUb-Bo0rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "wPVwBqQepIOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test data batch using the loaded full model (DRY Run)\n",
        "small_test_data = test_data.take(5)  # 5 batches only\n",
        "test_predictions = loaded_model.predict(small_test_data, verbose=1)"
      ],
      "metadata": {
        "id": "1NXb2vgUrhAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions (NumPy array) to csv file\n",
        "np.savetxt(\"preds_log.csv\", test_predictions, delimiter=\",\")"
      ],
      "metadata": {
        "id": "keuGLZOusSO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load predictions from csv file\n",
        "test_predictions = np.loadtxt(\"preds_log.csv\", delimiter=\",\")"
      ],
      "metadata": {
        "id": "DcQ1HhDRs4hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions[0]"
      ],
      "metadata": {
        "id": "d8BgYZGOtZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLWPxNwytcIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}